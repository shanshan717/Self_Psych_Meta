{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta-analytic functional decoding\n",
    "## Discrete functional decoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nimare\n",
    "import nibabel as nib\n",
    "from nilearn import datasets\n",
    "from nimare.stats import pearson\n",
    "from nimare.dataset import Dataset\n",
    "from nimare.decode import discrete\n",
    "from nimare.utils import get_resource_path\n",
    "from nimare.extract import fetch_neurosynth\n",
    "from nimare.io import convert_neurosynth_to_dataset\n",
    "from nimare.decode import continuous\n",
    "from nilearn import image\n",
    "from nilearn.plotting import plot_roi\n",
    "from nilearn.image import load_img\n",
    "from nilearn.image import new_img_like\n",
    "from nilearn import datasets, plotting\n",
    "from nilearn.masking import _unmask_3d\n",
    "from nilearn.maskers import nifti_spheres_masker\n",
    "from nibabel import Nifti1Image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a region of interest\n",
    "### 创建球形ROI\n",
    "\n",
    "根据疾病组和健康组元分析结果，创建相关的ROI\n",
    "\n",
    "疾病组： 创建一个brain mask：\n",
    "- L Frontal_Sup_Medial （-6，58，18）\n",
    "- L Cingulate_Post (-2, -54, 28)\n",
    "\n",
    "健康组： 创建五个brain mask\n",
    "- L Cingulate_Ant （0，48，8）\n",
    "- L OFCpost （-38，24，-14）\n",
    "- L Cingulate_Post （-2，-54，26）\n",
    "- L Angular（-44, -58，24）\n",
    "- R Cingulate_Mid （2， -16, 38）\n",
    "\n",
    "参考的neurostars 链接\n",
    "\n",
    "> https://neurostars.org/t/how-to-interpret-results-of-nimare-decode-discrete-roiassociationdecoder/30190"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's assume we are in MNI space\n",
    "brain_mask = datasets.load_mni152_brain_mask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# generate a mask for the left FPole\n",
    "_, A = nifti_spheres_masker._apply_mask_and_get_affinity(\n",
    "    seeds=[(-2, -54, 28)], \n",
    "    niimg=None,\n",
    "    radius=10,\n",
    "    allow_overlap=False, \n",
    "    mask_img=brain_mask)\n",
    "\n",
    "\n",
    "# Unmask the result (converts the sparse matrix to a 3D array)\n",
    "# _unmask_3d will convert the masked array into the shape of the original brain mask\n",
    "L_Cingulate_Post_patient_mask = _unmask_3d(\n",
    "    X=A.toarray().flatten(), \n",
    "    mask=brain_mask.get_fdata().astype(bool))\n",
    "\n",
    "L_Cingulate_Post_patient_roi = Nifti1Image(L_Cingulate_Post_patient_mask, brain_mask.affine)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn import plotting\n",
    "\n",
    "# 直接使用 cmap=\"Oranges\" 进行颜色设定\n",
    "plotting.plot_roi(L_Cingulate_Post_patient_roi, cmap=\"Wistia\")\n",
    "plotting.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_Cingulate_Post_patient_roi.to_filename('../Data/AnalysisData/ROI/L_Cingulate_Post_patient.nii.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 解码部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:nimare.extract.utils:Dataset found in ../Data/neurosynth\n",
      "\n",
      "INFO:nimare.extract.extract:Searching for any feature files matching the following criteria: [('data-neurosynth', 'version-7')]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data-neurosynth_version-7_coordinates.tsv.gz\n",
      "File exists and overwrite is False. Skipping.\n",
      "Downloading data-neurosynth_version-7_metadata.tsv.gz\n",
      "File exists and overwrite is False. Skipping.\n",
      "Downloading data-neurosynth_version-7_vocab-LDA100_keys.tsv\n",
      "File exists and overwrite is False. Skipping.\n",
      "Downloading data-neurosynth_version-7_vocab-LDA100_metadata.json\n",
      "File exists and overwrite is False. Skipping.\n",
      "Downloading data-neurosynth_version-7_vocab-LDA100_source-abstract_type-weight_features.npz\n",
      "File exists and overwrite is False. Skipping.\n",
      "Downloading data-neurosynth_version-7_vocab-LDA100_vocabulary.txt\n",
      "File exists and overwrite is False. Skipping.\n",
      "Downloading data-neurosynth_version-7_vocab-LDA200_keys.tsv\n",
      "File exists and overwrite is False. Skipping.\n",
      "Downloading data-neurosynth_version-7_vocab-LDA200_metadata.json\n",
      "File exists and overwrite is False. Skipping.\n",
      "Downloading data-neurosynth_version-7_vocab-LDA200_source-abstract_type-weight_features.npz\n",
      "File exists and overwrite is False. Skipping.\n",
      "Downloading data-neurosynth_version-7_vocab-LDA200_vocabulary.txt\n",
      "File exists and overwrite is False. Skipping.\n",
      "Downloading data-neurosynth_version-7_vocab-LDA400_keys.tsv\n",
      "File exists and overwrite is False. Skipping.\n",
      "Downloading data-neurosynth_version-7_vocab-LDA400_metadata.json\n",
      "File exists and overwrite is False. Skipping.\n",
      "Downloading data-neurosynth_version-7_vocab-LDA400_source-abstract_type-weight_features.npz\n",
      "File exists and overwrite is False. Skipping.\n",
      "Downloading data-neurosynth_version-7_vocab-LDA400_vocabulary.txt\n",
      "File exists and overwrite is False. Skipping.\n",
      "Downloading data-neurosynth_version-7_vocab-LDA50_keys.tsv\n",
      "File exists and overwrite is False. Skipping.\n",
      "Downloading data-neurosynth_version-7_vocab-LDA50_metadata.json\n",
      "File exists and overwrite is False. Skipping.\n",
      "Downloading data-neurosynth_version-7_vocab-LDA50_source-abstract_type-weight_features.npz\n",
      "File exists and overwrite is False. Skipping.\n",
      "Downloading data-neurosynth_version-7_vocab-LDA50_vocabulary.txt\n",
      "File exists and overwrite is False. Skipping.\n",
      "Downloading data-neurosynth_version-7_vocab-terms_source-abstract_type-tfidf_features.npz\n",
      "File exists and overwrite is False. Skipping.\n",
      "Downloading data-neurosynth_version-7_vocab-terms_vocabulary.txt\n",
      "File exists and overwrite is False. Skipping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:nimare.utils:Not applying transforms to coordinates in unrecognized space 'UNKNOWN'\n"
     ]
    }
   ],
   "source": [
    "# get neurosynth data\n",
    "databases = nimare.extract.fetch_neurosynth(data_dir='../Data')[0]\n",
    "\n",
    "# convert to NiMARE dataset (Note: This can take a while!)\n",
    "ds = nimare.io.convert_neurosynth_to_dataset(\n",
    "    coordinates_file=databases['coordinates'],\n",
    "    metadata_file=databases['metadata'],\n",
    "    annotations_files=databases['features']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>study_id</th>\n",
       "      <th>contrast_id</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>space</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1483</th>\n",
       "      <td>10022492-1</td>\n",
       "      <td>10022492</td>\n",
       "      <td>1</td>\n",
       "      <td>36.0</td>\n",
       "      <td>-58.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>mni152_2mm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1499</th>\n",
       "      <td>10022492-1</td>\n",
       "      <td>10022492</td>\n",
       "      <td>1</td>\n",
       "      <td>48.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>mni152_2mm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1498</th>\n",
       "      <td>10022492-1</td>\n",
       "      <td>10022492</td>\n",
       "      <td>1</td>\n",
       "      <td>-42.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>mni152_2mm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1497</th>\n",
       "      <td>10022492-1</td>\n",
       "      <td>10022492</td>\n",
       "      <td>1</td>\n",
       "      <td>-36.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>mni152_2mm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1496</th>\n",
       "      <td>10022492-1</td>\n",
       "      <td>10022492</td>\n",
       "      <td>1</td>\n",
       "      <td>-30.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>mni152_2mm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1479</th>\n",
       "      <td>9990082-1</td>\n",
       "      <td>9990082</td>\n",
       "      <td>1</td>\n",
       "      <td>42.0</td>\n",
       "      <td>-54.0</td>\n",
       "      <td>-21.0</td>\n",
       "      <td>mni152_2mm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1480</th>\n",
       "      <td>9990082-1</td>\n",
       "      <td>9990082</td>\n",
       "      <td>1</td>\n",
       "      <td>-36.0</td>\n",
       "      <td>-87.0</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>mni152_2mm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1481</th>\n",
       "      <td>9990082-1</td>\n",
       "      <td>9990082</td>\n",
       "      <td>1</td>\n",
       "      <td>30.0</td>\n",
       "      <td>-81.0</td>\n",
       "      <td>-15.0</td>\n",
       "      <td>mni152_2mm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1467</th>\n",
       "      <td>9990082-1</td>\n",
       "      <td>9990082</td>\n",
       "      <td>1</td>\n",
       "      <td>-18.0</td>\n",
       "      <td>-60.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>mni152_2mm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1482</th>\n",
       "      <td>9990082-1</td>\n",
       "      <td>9990082</td>\n",
       "      <td>1</td>\n",
       "      <td>-21.0</td>\n",
       "      <td>-78.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>mni152_2mm</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>507891 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id  study_id contrast_id     x     y     z       space\n",
       "1483  10022492-1  10022492           1  36.0 -58.0  52.0  mni152_2mm\n",
       "1499  10022492-1  10022492           1  48.0  24.0  20.0  mni152_2mm\n",
       "1498  10022492-1  10022492           1 -42.0  26.0  20.0  mni152_2mm\n",
       "1497  10022492-1  10022492           1 -36.0  30.0  16.0  mni152_2mm\n",
       "1496  10022492-1  10022492           1 -30.0  32.0   0.0  mni152_2mm\n",
       "...          ...       ...         ...   ...   ...   ...         ...\n",
       "1479   9990082-1   9990082           1  42.0 -54.0 -21.0  mni152_2mm\n",
       "1480   9990082-1   9990082           1 -36.0 -87.0  -6.0  mni152_2mm\n",
       "1481   9990082-1   9990082           1  30.0 -81.0 -15.0  mni152_2mm\n",
       "1467   9990082-1   9990082           1 -18.0 -60.0  54.0  mni152_2mm\n",
       "1482   9990082-1   9990082           1 -21.0 -78.0  27.0  mni152_2mm\n",
       "\n",
       "[507891 rows x 7 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coordinates columns: ['id', 'study_id', 'contrast_id', 'x', 'y', 'z', 'space']\n"
     ]
    }
   ],
   "source": [
    "print(\"Coordinates columns:\", ds.coordinates.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nimare.decode import discrete\n",
    "\n",
    "# decode ROI image\n",
    "\n",
    "for roi_name, roi_path in roi_files.items():\n",
    "    # load ROI image\n",
    "    roi_img = nib.load(roi_path)\n",
    "    \n",
    "    # generate decoder\n",
    "    decoder = discrete.ROIAssociationDecoder(\n",
    "        roi_img,\n",
    "        frequency_threshold=frequency_threshold,\n",
    "        u=u,\n",
    "        correction=correction,\n",
    "    )\n",
    "    \n",
    "    # fit decoder to dataset\n",
    "    decoder.fit(ds)\n",
    "    \n",
    "    # decoding and save to csv\n",
    "    decoded_ds = decoder.transform()\n",
    "    output_csv_path = f'../Output/6_Decoding/{roi_name}_decode.csv'\n",
    "    decoded_ds.to_csv(output_csv_path, index=True)\n",
    "\n",
    "    print(f\"Decoding for {roi_name} completed and saved to {output_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# 假设您有多个 CSV 文件存放在一个文件夹中\n",
    "input_folder = '../Output/6_Decoding'  # 修改为存放 CSV 文件的文件夹路径\n",
    "output_folder = '../Output/6_Decoding/clean_decoding'  # 修改为保存处理后的文件的文件夹路径\n",
    "\n",
    "# 确保输出文件夹存在\n",
    "os.makedirs(output_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 遍历文件夹中的所有 CSV 文件\n",
    "for file_name in os.listdir(input_folder):\n",
    "    if file_name.endswith('.csv'):  # 仅处理 CSV 文件\n",
    "        input_path = os.path.join(input_folder, file_name)\n",
    "        \n",
    "        # 读取 CSV 文件\n",
    "        df = pd.read_csv(input_path)\n",
    "        \n",
    "        # 筛选以 \"terms_abstract_tfidf__\" 开头的行\n",
    "        filtered_df = df[df['feature'].str.startswith('terms_abstract_tfidf__', na=False)]\n",
    "        \n",
    "        # 保存处理后的文件到输出文件夹\n",
    "        output_path = os.path.join(output_folder, file_name)\n",
    "        filtered_df.to_csv(output_path, index=False)\n",
    "        print(f'已处理文件: {file_name} 并保存到 {output_path}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用cognitive atlas的术语进行解码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Cognitive Atlas API base URL\n",
    "base_url = \"https://www.cognitiveatlas.org/api/v-alpha\"\n",
    "\n",
    "# Endpoints for concepts, tasks, and disorders\n",
    "concepts_endpoint = f\"{base_url}/concept?format=json\"\n",
    "tasks_endpoint = f\"{base_url}/task?format=json\"\n",
    "disorders_endpoint = f\"{base_url}/disorder?format=json\"\n",
    "\n",
    "\n",
    "# Fetch concepts and tasks data\n",
    "concepts_response = requests.get(concepts_endpoint)\n",
    "tasks_response = requests.get(tasks_endpoint)\n",
    "disorders_response = requests.get(disorders_endpoint)\n",
    "\n",
    "# Extract names from the response data\n",
    "concepts = concepts_response.json()\n",
    "tasks = tasks_response.json()\n",
    "disorders = disorders_response.json()\n",
    "\n",
    "# Get the names of concepts and tasks\n",
    "concept_names = [concept['name'] for concept in concepts]\n",
    "task_names = [task['name'] for task in tasks]\n",
    "disorder_names = [disorder['name'] for disorder in disorders]\n",
    "\n",
    "cognitive_atlas_terms = concept_names + task_names + disorder_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_to_keep = ['id'] + [term for term in ds.annotations.columns if term.split('__')[-1] in cognitive_atlas_terms]\n",
    "ds.annotations = ds.annotations[terms_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roi_files = {\n",
    "    \"L_Angular\": \"../Data/AnalysisData/ROI/L_Angular.nii.gz\",\n",
    "    \"L_Cingulate_Ant\": \"../Data/AnalysisData/ROI/L_Cingulate_Ant.nii.gz\",\n",
    "    \"L_Cingulate_Post_patient\": \"../Data/AnalysisData/ROI/L_Cingulate_Post_patient.nii.gz\",\n",
    "    \"L_Cingulate_Post\": \"../Data/AnalysisData/ROI/L_Cingulate_Post.nii.gz\",\n",
    "    \"L_Frontal_Sup_Medial\": \"../Data/AnalysisData/ROI/L_Frontal_Sup_Medial.nii.gz\",\n",
    "    \"L_OFCpost\": \"../Data/AnalysisData/ROI/L_OFCpost.nii.gz\",\n",
    "    \"R_Cingulate_Mid\": \"../Data/AnalysisData/ROI/R_Cingulate_Mid.nii.gz\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_Angular = \"../Data/AnalysisData/ROI/L_Angular.nii.gz\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_Cingulate_Ant= \"/Users/ss/Documents/Self_Psych_Meta/Data/AnalysisData/ROI/L_Cingulate_Ant.nii.gz\",\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_Cingulate_Post_patient = \"../Data/AnalysisData/ROI/L_Cingulate_Post_patient.nii.gz\",\n",
    "L_Cingulate_Post =\"../Data/AnalysisData/ROI/L_Cingulate_Post.nii.gz\",\n",
    "L_Frontal_Sup_Medial = \"../Data/AnalysisData/ROI/L_Frontal_Sup_Medial.nii.gz\",\n",
    "L_OFCpost = \"../Data/AnalysisData/ROI/L_OFCpost.nii.gz\",\n",
    "R_Cingulate_Mid = \"../Data/AnalysisData/ROI/R_Cingulate_Mid.nii.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define decoder parameters\n",
    "frequency_threshold = 0.001\n",
    "u = 0.05\n",
    "correction = \"fdr_bh\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nimare.decode.continuous import CorrelationDecoder\n",
    "from nimare.meta.cbma import mkda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = CorrelationDecoder(L_Cingulate_Ant, \n",
    "                             frequency_threshold=0.001, \n",
    "                             meta_estimator=mkda.MKDAChi2,\n",
    "                             target_image='z_desc-association')\n",
    "decoder.fit(ds)\n",
    "decoded_df = decoder.transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_df.to_csv('/Users/ss/Documents/Self_Psych_Meta/Output/6_Decoding/L_Angular.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot word cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib\n",
    "\n",
    "# 定义输入文件列表\n",
    "csv_files = [\n",
    "    '../results/extract_decoding/extract_Angular_decode.csv',\n",
    "    '../results/extract_decoding/extract_Cingulate_decode.csv',\n",
    "    '../results/extract_decoding/extract_FPole_decode.csv',\n",
    "    '../results/extract_decoding/extract_L_FOC_decode.csv',\n",
    "    '../results/extract_decoding/extract_R_FOC_decode.csv',\n",
    "    '../results/extract_decoding/extract_PCG_decode.csv'\n",
    "]\n",
    "\n",
    "# 定义输出目录\n",
    "output_dir = '../results/wordcloud'\n",
    "os.makedirs(output_dir, exist_ok=True)  # 如果目录不存在，则创建\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 批量处理每个 CSV 文件\n",
    "# \"viridis\" 绿色\n",
    "for file_path in csv_files:\n",
    "    # 读取 CSV 数据\n",
    "    data = pd.read_csv(file_path)\n",
    "\n",
    "    # 构建词频字典，使用 'cognitive_feature' 作为单词，'r' 作为词频\n",
    "    word_frequencies = data.set_index('cognitive_feature')['r'].to_dict()\n",
    "\n",
    "    # 创建词云\n",
    "    wordcloud = WordCloud(\n",
    "        width=800,\n",
    "        height=400,\n",
    "        colormap=\"YlOrRd\",\n",
    "        background_color='white',\n",
    "        prefer_horizontal=1.0\n",
    "    ).generate_from_frequencies(word_frequencies)\n",
    "\n",
    "    # 绘制并显示词云图\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')  # 关闭坐标轴\n",
    "    plt.show()\n",
    "\n",
    "    # 保存词云图为文件\n",
    "    file_name = os.path.basename(file_path).replace('extract_', '').replace('_decode.csv', '')  # 提取文件名的一部分\n",
    "    output_path = os.path.join(output_dir, f'{file_name}_wordcloud.png')\n",
    "    wordcloud.to_file(output_path)\n",
    "\n",
    "    print(f\"词云图已保存到 {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
