{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta-analytic functional decoding\n",
    "## Discrete functional decoding\n",
    "\n",
    "In this notebook, we applied meta-analytic functional decoding using the NiMARE (https://nimare.readthedocs.io/en/stable/about.html). Specifically, we used the discrete decoding approach, which estimates the statistical association between brain regions and cognitive terms based on large-scale meta-analytic databases (e.g., Neurosynth).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nibabel as nib\n",
    "from nibabel import Nifti1Image\n",
    "\n",
    "from nilearn import datasets, image, plotting\n",
    "from nilearn.plotting import plot_roi\n",
    "from nilearn.image import load_img\n",
    "from nilearn.masking import _unmask_3d\n",
    "from nilearn.maskers import nifti_spheres_masker\n",
    "\n",
    "import nimare\n",
    "from nimare.stats import pearson\n",
    "from nimare.dataset import Dataset\n",
    "from nimare.decode import discrete, continuous\n",
    "from nimare.utils import get_resource_path\n",
    "from nimare.extract import fetch_neurosynth\n",
    "from nimare.io import convert_neurosynth_to_dataset\n",
    "\n",
    "# print the current working directory\n",
    "# print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:nimare.extract.utils:Dataset found in ../Data/neurosynth\n",
      "\n",
      "INFO:nimare.extract.extract:Searching for any feature files matching the following criteria: [('data-neurosynth', 'version-7')]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data-neurosynth_version-7_coordinates.tsv.gz\n",
      "File exists and overwrite is False. Skipping.\n",
      "Downloading data-neurosynth_version-7_metadata.tsv.gz\n",
      "File exists and overwrite is False. Skipping.\n",
      "Downloading data-neurosynth_version-7_vocab-LDA100_keys.tsv\n",
      "File exists and overwrite is False. Skipping.\n",
      "Downloading data-neurosynth_version-7_vocab-LDA100_metadata.json\n",
      "File exists and overwrite is False. Skipping.\n",
      "Downloading data-neurosynth_version-7_vocab-LDA100_source-abstract_type-weight_features.npz\n",
      "File exists and overwrite is False. Skipping.\n",
      "Downloading data-neurosynth_version-7_vocab-LDA100_vocabulary.txt\n",
      "File exists and overwrite is False. Skipping.\n",
      "Downloading data-neurosynth_version-7_vocab-LDA200_keys.tsv\n",
      "File exists and overwrite is False. Skipping.\n",
      "Downloading data-neurosynth_version-7_vocab-LDA200_metadata.json\n",
      "File exists and overwrite is False. Skipping.\n",
      "Downloading data-neurosynth_version-7_vocab-LDA200_source-abstract_type-weight_features.npz\n",
      "File exists and overwrite is False. Skipping.\n",
      "Downloading data-neurosynth_version-7_vocab-LDA200_vocabulary.txt\n",
      "File exists and overwrite is False. Skipping.\n",
      "Downloading data-neurosynth_version-7_vocab-LDA400_keys.tsv\n",
      "File exists and overwrite is False. Skipping.\n",
      "Downloading data-neurosynth_version-7_vocab-LDA400_metadata.json\n",
      "File exists and overwrite is False. Skipping.\n",
      "Downloading data-neurosynth_version-7_vocab-LDA400_source-abstract_type-weight_features.npz\n",
      "File exists and overwrite is False. Skipping.\n",
      "Downloading data-neurosynth_version-7_vocab-LDA400_vocabulary.txt\n",
      "File exists and overwrite is False. Skipping.\n",
      "Downloading data-neurosynth_version-7_vocab-LDA50_keys.tsv\n",
      "File exists and overwrite is False. Skipping.\n",
      "Downloading data-neurosynth_version-7_vocab-LDA50_metadata.json\n",
      "File exists and overwrite is False. Skipping.\n",
      "Downloading data-neurosynth_version-7_vocab-LDA50_source-abstract_type-weight_features.npz\n",
      "File exists and overwrite is False. Skipping.\n",
      "Downloading data-neurosynth_version-7_vocab-LDA50_vocabulary.txt\n",
      "File exists and overwrite is False. Skipping.\n",
      "Downloading data-neurosynth_version-7_vocab-terms_source-abstract_type-tfidf_features.npz\n",
      "File exists and overwrite is False. Skipping.\n",
      "Downloading data-neurosynth_version-7_vocab-terms_vocabulary.txt\n",
      "File exists and overwrite is False. Skipping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:nimare.utils:Not applying transforms to coordinates in unrecognized space 'UNKNOWN'\n"
     ]
    }
   ],
   "source": [
    "# get neurosynth data\n",
    "databases = nimare.extract.fetch_neurosynth(data_dir='../Data')[0]\n",
    "\n",
    "# convert to NiMARE dataset (Note: This can take a while!)\n",
    "ds = nimare.io.convert_neurosynth_to_dataset(\n",
    "    coordinates_file=databases['coordinates'],\n",
    "    metadata_file=databases['metadata'],\n",
    "    annotations_files=databases['features']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>study_id</th>\n",
       "      <th>contrast_id</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>space</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1483</th>\n",
       "      <td>10022492-1</td>\n",
       "      <td>10022492</td>\n",
       "      <td>1</td>\n",
       "      <td>36.0</td>\n",
       "      <td>-58.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>mni152_2mm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1499</th>\n",
       "      <td>10022492-1</td>\n",
       "      <td>10022492</td>\n",
       "      <td>1</td>\n",
       "      <td>48.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>mni152_2mm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1498</th>\n",
       "      <td>10022492-1</td>\n",
       "      <td>10022492</td>\n",
       "      <td>1</td>\n",
       "      <td>-42.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>mni152_2mm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1497</th>\n",
       "      <td>10022492-1</td>\n",
       "      <td>10022492</td>\n",
       "      <td>1</td>\n",
       "      <td>-36.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>mni152_2mm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1496</th>\n",
       "      <td>10022492-1</td>\n",
       "      <td>10022492</td>\n",
       "      <td>1</td>\n",
       "      <td>-30.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>mni152_2mm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1479</th>\n",
       "      <td>9990082-1</td>\n",
       "      <td>9990082</td>\n",
       "      <td>1</td>\n",
       "      <td>42.0</td>\n",
       "      <td>-54.0</td>\n",
       "      <td>-21.0</td>\n",
       "      <td>mni152_2mm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1480</th>\n",
       "      <td>9990082-1</td>\n",
       "      <td>9990082</td>\n",
       "      <td>1</td>\n",
       "      <td>-36.0</td>\n",
       "      <td>-87.0</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>mni152_2mm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1481</th>\n",
       "      <td>9990082-1</td>\n",
       "      <td>9990082</td>\n",
       "      <td>1</td>\n",
       "      <td>30.0</td>\n",
       "      <td>-81.0</td>\n",
       "      <td>-15.0</td>\n",
       "      <td>mni152_2mm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1467</th>\n",
       "      <td>9990082-1</td>\n",
       "      <td>9990082</td>\n",
       "      <td>1</td>\n",
       "      <td>-18.0</td>\n",
       "      <td>-60.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>mni152_2mm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1482</th>\n",
       "      <td>9990082-1</td>\n",
       "      <td>9990082</td>\n",
       "      <td>1</td>\n",
       "      <td>-21.0</td>\n",
       "      <td>-78.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>mni152_2mm</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>507891 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id  study_id contrast_id     x     y     z       space\n",
       "1483  10022492-1  10022492           1  36.0 -58.0  52.0  mni152_2mm\n",
       "1499  10022492-1  10022492           1  48.0  24.0  20.0  mni152_2mm\n",
       "1498  10022492-1  10022492           1 -42.0  26.0  20.0  mni152_2mm\n",
       "1497  10022492-1  10022492           1 -36.0  30.0  16.0  mni152_2mm\n",
       "1496  10022492-1  10022492           1 -30.0  32.0   0.0  mni152_2mm\n",
       "...          ...       ...         ...   ...   ...   ...         ...\n",
       "1479   9990082-1   9990082           1  42.0 -54.0 -21.0  mni152_2mm\n",
       "1480   9990082-1   9990082           1 -36.0 -87.0  -6.0  mni152_2mm\n",
       "1481   9990082-1   9990082           1  30.0 -81.0 -15.0  mni152_2mm\n",
       "1467   9990082-1   9990082           1 -18.0 -60.0  54.0  mni152_2mm\n",
       "1482   9990082-1   9990082           1 -21.0 -78.0  27.0  mni152_2mm\n",
       "\n",
       "[507891 rows x 7 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用cognitive atlas的术语进行解码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching Cognitive Atlas terms...\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "print(\"Fetching Cognitive Atlas terms...\")\n",
    "base_url = \"https://www.cognitiveatlas.org/api/v-alpha\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 1771 cognitive terms from Cognitive Atlas.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "concepts = requests.get(f\"{base_url}/concept?format=json\").json()\n",
    "tasks = requests.get(f\"{base_url}/task?format=json\").json()\n",
    "\n",
    "cognitive_terms = [c['name'].lower() for c in concepts] + [t['name'].lower() for t in tasks]\n",
    "\n",
    "print(f\"Fetched {len(cognitive_terms)} cognitive terms from Cognitive Atlas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering annotations to retain only cognitive terms...\n",
      "Remaining features after filtering: 133\n"
     ]
    }
   ],
   "source": [
    "print(\"Filtering annotations to retain only cognitive terms...\")\n",
    "terms_to_keep = ['id'] + [\n",
    "    col for col in ds.annotations.columns\n",
    "    if col.split('__')[-1].lower() in cognitive_terms\n",
    "]\n",
    "\n",
    "ds.annotations = ds.annotations[terms_to_keep]\n",
    "print(f\"Remaining features after filtering: {len(ds.annotations.columns) - 1}\")\n",
    "\n",
    "# 🧩 关键修复部分\n",
    "if 'study_id' not in ds.annotations.columns:\n",
    "    ds.annotations['study_id'] = ds.annotations['id']\n",
    "if 'contrast_id' not in ds.annotations.columns:\n",
    "    ds.annotations['contrast_id'] = ds.annotations['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running continuous decoding for /Users/ss/Documents/Self_Psych_Meta/Output/1_ALE/control_all_z_thresh.nii.gz...\n"
     ]
    }
   ],
   "source": [
    "from nimare.decode import continuous\n",
    "\n",
    "zmap_path = \"/Users/ss/Documents/Self_Psych_Meta/Output/1_ALE/control_all_z_thresh.nii.gz\"  # <-- 修改为你的 z-map 路径\n",
    "\n",
    "print(f\"Running continuous decoding for {zmap_path}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define decoder parameters\n",
    "frequency_threshold = 0.001\n",
    "u = 0.05\n",
    "correction = \"fdr_bh\"\n",
    "\n",
    "decoder = continuous.CorrelationDecoder(\n",
    "    frequency_threshold=frequency_threshold,\n",
    "    target_image=zmap_path\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/133 [00:19<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No map with name '/Users/ss/Documents/Self_Psych_Meta/Output/1_ALE/control_all_z_thresh.nii.gz' found.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m decoded_df \u001b[38;5;241m=\u001b[39m decoder\u001b[38;5;241m.\u001b[39mtransform()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/nimare/decode/base.py:108\u001b[0m, in \u001b[0;36mDecoder.fit\u001b[0;34m(self, dataset, drop_invalid)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_collect_inputs(dataset, drop_invalid\u001b[38;5;241m=\u001b[39mdrop_invalid)\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_preprocess_input(dataset)\n\u001b[0;32m--> 108\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/nimare/decode/continuous.py:199\u001b[0m, in \u001b[0;36mCorrelationDecoder._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Generate feature-specific meta-analytic maps for dataset.\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \n\u001b[1;32m    186\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;124;03m    MetaResult with meta-analytic maps and masker added.\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    196\u001b[0m n_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures_)\n\u001b[1;32m    197\u001b[0m maps \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    198\u001b[0m     r: v\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m r, v \u001b[38;5;129;01min\u001b[39;00m tqdm(\n\u001b[1;32m    200\u001b[0m         Parallel(return_as\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator\u001b[39m\u001b[38;5;124m\"\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_cores)(\n\u001b[1;32m    201\u001b[0m             delayed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_fit)(feature, dataset) \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures_\n\u001b[1;32m    202\u001b[0m         ),\n\u001b[1;32m    203\u001b[0m         total\u001b[38;5;241m=\u001b[39mn_features,\n\u001b[1;32m    204\u001b[0m     )\n\u001b[1;32m    205\u001b[0m }\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults_ \u001b[38;5;241m=\u001b[39m MetaResult(\u001b[38;5;28mself\u001b[39m, mask\u001b[38;5;241m=\u001b[39mdataset\u001b[38;5;241m.\u001b[39mmasker, maps\u001b[38;5;241m=\u001b[39mmaps)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[1;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/joblib/parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/nimare/decode/continuous.py:229\u001b[0m, in \u001b[0;36mCorrelationDecoder._run_fit\u001b[0;34m(self, feature, dataset)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    227\u001b[0m     meta_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeta_estimator\u001b[38;5;241m.\u001b[39mfit(feature_dset)\n\u001b[0;32m--> 229\u001b[0m feature_data \u001b[38;5;241m=\u001b[39m \u001b[43mmeta_results\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_map\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_image\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43marray\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m feature, feature_data\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/nimare/results.py:132\u001b[0m, in \u001b[0;36mMetaResult.get_map\u001b[0;34m(self, name, return_type)\u001b[0m\n\u001b[1;32m    130\u001b[0m m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaps\u001b[38;5;241m.\u001b[39mget(name)\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 132\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo map with name \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m found.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;66;03m# pending resolution of https://github.com/nilearn/nilearn/issues/2724\u001b[39;00m\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: No map with name '/Users/ss/Documents/Self_Psych_Meta/Output/1_ALE/control_all_z_thresh.nii.gz' found."
     ]
    }
   ],
   "source": [
    "decoder.fit(ds)\n",
    "decoded_df = decoder.transform()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "decoder.fit(ds)\n",
    "decoded_df = decoder.transform()\n",
    "\n",
    "# Save results\n",
    "out_csv = \"zmap_decoding_results.csv\"\n",
    "decoded_df.to_csv(out_csv, index=False)\n",
    "print(f\"Decoding results saved to {out_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "SSLError",
     "evalue": "HTTPSConnectionPool(host='www.cognitiveatlas.org', port=443): Max retries exceeded with url: /api/v-alpha/concept?format=json (Caused by SSLError(SSLEOFError(8, '[SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1010)')))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSSLError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;31mSSLError\u001b[0m: [SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1010)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/urllib3/connectionpool.py:841\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    839\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[0;32m--> 841\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    844\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/urllib3/util/retry.py:519\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    518\u001b[0m     reason \u001b[38;5;241m=\u001b[39m error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause)\n\u001b[0;32m--> 519\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mreason\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    521\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='www.cognitiveatlas.org', port=443): Max retries exceeded with url: /api/v-alpha/concept?format=json (Caused by SSLError(SSLEOFError(8, '[SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1010)')))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mSSLError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 11\u001b[0m\n\u001b[1;32m      7\u001b[0m disorders_endpoint \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/disorder?format=json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Fetch concepts and tasks data\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m concepts_response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconcepts_endpoint\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m tasks_response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(tasks_endpoint)\n\u001b[1;32m     13\u001b[0m disorders_response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(disorders_endpoint)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/requests/adapters.py:698\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    694\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ProxyError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m    696\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, _SSLError):\n\u001b[1;32m    697\u001b[0m         \u001b[38;5;66;03m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[0;32m--> 698\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClosedPoolError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mSSLError\u001b[0m: HTTPSConnectionPool(host='www.cognitiveatlas.org', port=443): Max retries exceeded with url: /api/v-alpha/concept?format=json (Caused by SSLError(SSLEOFError(8, '[SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1010)')))"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Cognitive Atlas API base URL\n",
    "base_url = \"https://www.cognitiveatlas.org/api/v-alpha\"\n",
    "\n",
    "# Endpoints for concepts, tasks, and disorders\n",
    "concepts_endpoint = f\"{base_url}/concept?format=json\"\n",
    "tasks_endpoint = f\"{base_url}/task?format=json\"\n",
    "disorders_endpoint = f\"{base_url}/disorder?format=json\"\n",
    "\n",
    "\n",
    "# Fetch concepts and tasks data\n",
    "concepts_response = requests.get(concepts_endpoint)\n",
    "tasks_response = requests.get(tasks_endpoint)\n",
    "disorders_response = requests.get(disorders_endpoint)\n",
    "\n",
    "# Extract names from the response data\n",
    "concepts = concepts_response.json()\n",
    "tasks = tasks_response.json()\n",
    "disorders = disorders_response.json()\n",
    "\n",
    "# Get the names of concepts and tasks\n",
    "concept_names = [concept['name'] for concept in concepts]\n",
    "task_names = [task['name'] for task in tasks]\n",
    "disorder_names = [disorder['name'] for disorder in disorders]\n",
    "\n",
    "cognitive_atlas_terms = concept_names + task_names + disorder_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_to_keep = ['id'] + [\n",
    "    term for term in ds.annotations.columns \n",
    "    if term.split('__')[-1].replace('_', ' ') in cognitive_atlas_terms\n",
    "]\n",
    "\n",
    "ds.annotations = ds.annotations[terms_to_keep]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds.annotations.columns[:10])\n",
    "print(cognitive_atlas_terms[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define decoder parameters\n",
    "frequency_threshold = 0.001\n",
    "u = 0.05\n",
    "correction = \"fdr_bh\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nimare.decode.continuous import CorrelationDecoder\n",
    "from nimare.meta.cbma import mkda\n",
    "from nilearn.image import load_img\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ale_img_path = \"/Users/ss/Documents/Self_Psych_Meta/Output/1_ALE/control_all_z_thresh.nii\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = CorrelationDecoder(ale_img_path,\n",
    "    frequency_threshold=frequency_threshold,\n",
    "    meta_estimator=mkda.MKDAChi2,\n",
    "    target_image='z_desc-association'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "decoder.fit(ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "decoded_df = decoder.transform()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.makedirs('/Users/ss/Documents/Self_Psych_Meta/Output/6_Decoding', exist_ok=True)\n",
    "decoded_df.to_csv('/Users/ss/Documents/Self_Psych_Meta/Output/6_Decoding/control_all_decode.csv', index=True)\n",
    "# ...existing code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "decoder.fit(ds)\n",
    "decoded_df = decoder.transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_df.to_csv('/Users/ss/Documents/Self_Psych_Meta/Output/6_Decoding/L_Angular.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot word cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib\n",
    "\n",
    "# 定义输入文件列表\n",
    "csv_files = [\n",
    "    '../results/extract_decoding/extract_Angular_decode.csv',\n",
    "    '../results/extract_decoding/extract_Cingulate_decode.csv',\n",
    "    '../results/extract_decoding/extract_FPole_decode.csv',\n",
    "    '../results/extract_decoding/extract_L_FOC_decode.csv',\n",
    "    '../results/extract_decoding/extract_R_FOC_decode.csv',\n",
    "    '../results/extract_decoding/extract_PCG_decode.csv'\n",
    "]\n",
    "\n",
    "# 定义输出目录\n",
    "output_dir = '../results/wordcloud'\n",
    "os.makedirs(output_dir, exist_ok=True)  # 如果目录不存在，则创建\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 定义输入文件列表\n",
    "csv_files = [\n",
    "    '/Users/ss/Documents/Self_Psych_Meta/Output/wordcloud/control_clean.csv',\n",
    "    '/Users/ss/Documents/Self_Psych_Meta/Output/wordcloud/control_minus_patient_clean.csv',\n",
    "    '/Users/ss/Documents/Self_Psych_Meta/Output/wordcloud/patient_clean.csv',\n",
    "    '/Users/ss/Documents/Self_Psych_Meta/Output/wordcloud/patient_minus_control_clean.csv',\n",
    "    '/Users/ss/Documents/Self_Psych_Meta/Output/wordcloud/sz_clean.csv'\n",
    "]\n",
    "\n",
    "# 定义输出目录\n",
    "output_dir = '/Users/ss/Documents/Self_Psych_Meta/Output/wordcloud'\n",
    "os.makedirs(output_dir, exist_ok=True)  # 如果目录不存在，则创建\n",
    "\n",
    "\n",
    "# 批量处理每个 CSV 文件\n",
    "for file_path in csv_files:\n",
    "    # 读取 CSV 数据\n",
    "    data = pd.read_csv(file_path)\n",
    "\n",
    "    # 构建词频字典，使用 'feature' 作为单词，'r' 作为词频\n",
    "    word_frequencies = data.set_index('feature')['r'].to_dict()\n",
    "\n",
    "    # 创建词云\n",
    "    wordcloud = WordCloud(\n",
    "        width=800,\n",
    "        height=400,\n",
    "        colormap=\"Reds\",\n",
    "        background_color='white',\n",
    "        max_words=150,\n",
    "        max_font_size=150,\n",
    "        scale=3,\n",
    "        relative_scaling=0.5,\n",
    "        prefer_horizontal=1.0\n",
    "    ).generate_from_frequencies(word_frequencies)\n",
    "\n",
    "    # 绘制并显示词云图\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')  # 关闭坐标轴\n",
    "\n",
    "    # 获取'r'值的范围，用于color bar\n",
    "    r_values = list(word_frequencies.values())\n",
    "    vmin, vmax = min(r_values), max(r_values)\n",
    "\n",
    "    # 创建一个标量映射对象，用于color bar\n",
    "    norm = plt.Normalize(vmin=vmin, vmax=vmax)\n",
    "    sm = plt.cm.ScalarMappable(cmap=\"Reds\", norm=norm)\n",
    "    sm.set_array([])  \n",
    "\n",
    "    # 添加color bar\n",
    "    cbar = plt.colorbar(sm, ax=plt.gca(), orientation='vertical', fraction=0.046, pad=0.04)\n",
    "    cbar.set_label('', rotation=270, labelpad=15)  # 设置color bar的标签\n",
    "\n",
    "    # 显示图像\n",
    "    plt.show()\n",
    "\n",
    "    # 保存词云图为文件\n",
    "    file_name = os.path.basename(file_path).replace('extract_', '').replace('_decode.csv', '')  # 提取文件名的一部分\n",
    "    output_path = os.path.join(output_dir, f'{file_name}_wordcloud.png')\n",
    "    wordcloud.to_file(output_path)\n",
    "\n",
    "    print(f\"词云图已保存到 {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 批量处理每个 CSV 文件\n",
    "# \"viridis\" 绿色\n",
    "for file_path in csv_files:\n",
    "    # 读取 CSV 数据\n",
    "    data = pd.read_csv(file_path)\n",
    "\n",
    "    # 构建词频字典，使用 'cognitive_feature' 作为单词，'r' 作为词频\n",
    "    word_frequencies = data.set_index('cognitive_feature')['r'].to_dict()\n",
    "\n",
    "    # 创建词云\n",
    "    wordcloud = WordCloud(\n",
    "        width=800,\n",
    "        height=400,\n",
    "        colormap=\"YlOrRd\",\n",
    "        background_color='white',\n",
    "        prefer_horizontal=1.0\n",
    "    ).generate_from_frequencies(word_frequencies)\n",
    "\n",
    "    # 绘制并显示词云图\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')  # 关闭坐标轴\n",
    "    plt.show()\n",
    "\n",
    "    # 保存词云图为文件\n",
    "    file_name = os.path.basename(file_path).replace('extract_', '').replace('_decode.csv', '')  # 提取文件名的一部分\n",
    "    output_path = os.path.join(output_dir, f'{file_name}_wordcloud.png')\n",
    "    wordcloud.to_file(output_path)\n",
    "\n",
    "    print(f\"词云图已保存到 {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
